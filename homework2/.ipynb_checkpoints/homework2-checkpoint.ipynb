{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 344 Homework 2\n",
    "## Elizabeth Koning\n",
    "March 7, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i'] 0.5\n",
      "0.5\n",
      "['blah'] 0.4\n",
      "0.4\n",
      "['spamiam', 'am'] 0.9850746268656716\n",
      "0.9850746268656716\n",
      "['i', 'blah'] 0.4\n",
      "0.4\n",
      "['i', 'am'] 0.99\n",
      "0.99\n",
      "['i', 'do'] 0.3333333333333333\n",
      "0.3333333333333333\n",
      "['i', 'am', 'spam', 'spam', 'i', 'am'] 0.9999999895897965\n",
      "0.9999999895897965\n",
      "['do', 'i', 'like', 'green', 'eggs', 'and', 'ham'] 2.6025508824397714e-09\n",
      "2.6025508824397714e-09\n"
     ]
    }
   ],
   "source": [
    "spam_corpus = [[\"i\", \"am\", \"spam\", \"spam\", \"i\", \"am\"], [\"i\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "\n",
    "min_threshold = 1\n",
    "unseen_prob = 0.4\n",
    "\n",
    "\n",
    "class SpamFilter:\n",
    "\n",
    "    def __init__(self, spam, not_spam):\n",
    "        # number of messages in each corpus\n",
    "        self.ngood = len(not_spam)\n",
    "        self.nbad  = len(spam)\n",
    "\n",
    "        # flatten lists\n",
    "        spam = sum(spam, [])\n",
    "        not_spam = sum(not_spam, [])\n",
    "\n",
    "        # list of words in either corpus\n",
    "        keys = list( set(spam) | set(not_spam) ) # union of key lists\n",
    "\n",
    "        # store counts of tokens in each list\n",
    "        self.spam = {token:spam.count(token) for token in keys}\n",
    "        self.not_spam = {token:not_spam.count(token) for token in keys}\n",
    "\n",
    "        # third hash table -- probability that message containing word is spam\n",
    "        self.probs = {}\n",
    "        for token in keys:\n",
    "            g = float(2 * self.not_spam[token])\n",
    "            b = float(self.spam[token])\n",
    "            if g + b > min_threshold:\n",
    "                self.probs[token] = max(0.01, min(0.99, min(1.0, b/self.nbad) / (min(1.0, g/self.ngood) + min(1.0, b/self.nbad))))\n",
    "\n",
    "    def filter(self, text):\n",
    "        product = 1.0\n",
    "        comp_product = 1.0\n",
    "\n",
    "        for token in text:\n",
    "            if token in self.probs:\n",
    "                probability = self.probs[token]\n",
    "            else:\n",
    "                probability = unseen_prob\n",
    "            product *= probability\n",
    "            comp_product *= (1.0 - probability)\n",
    "\n",
    "        print(text, product / (product + comp_product))\n",
    "\n",
    "        return product / (product + comp_product)\n",
    "    \n",
    "    \n",
    "spam_filter = SpamFilter(spam_corpus, ham_corpus)\n",
    "\n",
    "# examples\n",
    "print(spam_filter.filter([\"i\"])) # single word\n",
    "print(spam_filter.filter([\"blah\"])) # single word not in corpuses\n",
    "print(spam_filter.filter([\"spamiam\", \"am\"]))\n",
    "print(spam_filter.filter([\"i\", \"blah\"]))\n",
    "print(spam_filter.filter([\"i\", \"am\"]))\n",
    "print(spam_filter.filter([\"i\", \"do\"])) # ham list\n",
    "print(spam_filter.filter([\"i\", \"am\", \"spam\", \"spam\", \"i\", \"am\"])) # spam list\n",
    "print(spam_filter.filter([\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"])) # ham list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes this approach to SPAM Bayesian?\n",
    "\n",
    "This approach is Bayesian because of how we are handling probabilities. We are dealing with our degree of belief that the given message is spam.\n",
    "\n",
    "This system works with probabilities instead of hard rules. In his post, Graham explains the advantages of that, but it relates to Bayesian statistics in how the result of our analysis is probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-02c744a571fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mprobability\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBayesNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menumeration_ask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melimination_ask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgibbs_ask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m# Utility variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m grass = BayesNet([\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'probability'"
     ]
    }
   ],
   "source": [
    "from probability import BayesNet, enumeration_ask, elimination_ask, gibbs_ask\n",
    "\n",
    "# Utility variables\n",
    "\n",
    "grass = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T:0.1, F:0.5}),\n",
    "    ('Rain', 'Cloudy', {T:0.8, F:0.2}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T):0.99, (T,F):0.9, (F,T):0.9, (F,F):0.0}),\n",
    "    ])\n",
    "\n",
    "print(\"i. \" + enumeration_ask('Cloudy', dict(), grass).show_approx())\n",
    "\n",
    "print(\"ii. \" + enumeration_ask('Sprinkler', dict(Cloudy=T), grass).show_approx())\n",
    "\n",
    "print(\"iii. \" + enumeration_ask('Cloudy', dict(Sprinkler=T, Rain=F), grass).show_approx())\n",
    "\n",
    "print(\"iv. \" + enumeration_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), grass).show_approx())\n",
    "\n",
    "print(\"v. \" + enumeration_ask('Cloudy', dict(WetGrass=F), grass).show_approx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b\n",
    "2^4 = 16\n",
    "This number of independent values in the full joint probability distrubtion comes from the 4 different variables, each of which can be true or false.\n",
    "\n",
    "\n",
    "### 2c\n",
    "9\n",
    "The number of independent values in the Bayesian network can be counted from the multiply connected network as shown in the figure.\n",
    "\n",
    "### 2d\n",
    "i. P(Cloudy) = <0.5, 0.5>\n",
    "\n",
    "ii. P(Sprinker | cloudy) = <0.1, 0.9>\n",
    "\n",
    "iii. P(Cloudy| the sprinkler is running and it’s not raining) = alpha * <0.5*0.1*0.2, 0.5*0.5*0.8> = alpha * <0.01, 0.2> = <0.0476, 0.9524>\n",
    "\n",
    "iv. P(WetGrass | it’s cloudy, the sprinkler is running and it’s raining) = alpha * <0.5*0.1*0.8*0.99, 0.5*0.1*0.8*0.01> = alpha * <0.99, 0.01> = <0.99, 0.01>\n",
    "\n",
    "v. P(Cloudy | not GrassWet) = alpha * sum( P(C, r, s, not g) )\n",
    "\n",
    "    = alpha * sum(s)( sum(r)( P(C)*P(s^r)*P(g | s^r) ) )\n",
    "\n",
    "\n",
    "       s ^ r       TT              TF              FT            FF\n",
    "\n",
    "    = alpha * <0.5*0.08*0.01 + 0.5*0.02*0.10 + 0.5*0.72*0.10 + 0.5*0.18*1.00, C\n",
    "    \n",
    "               0.5*0.10*0.01 + 0.5*0.40*0.10 + 0.5*0.10*0.10 + 0.5*0.40*1.00> not C\n",
    "\n",
    "    = alpha * <0.1274, 0.2255>\n",
    "    \n",
    "    = <0.361, 0.639>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
