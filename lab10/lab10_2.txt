CS 344 Lab 10
Elizabeth Koning
Exercise 2

a. What does AdaGrad do to boost performance?

AdaGrad adapts the learning rate for each coefficient in the model. This is similarly to normalizing the data on a linear scale. It decreases the learning rate, so it is helpful to use a larger learning rate if using AdaGrad.

b. Tasks 1â€“4: Report your best hyperparameter settings and their resulting performance.

Task 1 -- this is what the normalize_linear_scale() function looked like:
  def normalize_linear_scale(examples_dataframe):
    """Returns a version of the input `DataFrame` that has all its features normalized linearly."""
    processed_features = pd.DataFrame()
    processed_features["latitude"] = linear_scale(examples_dataframe["latitude"])
    processed_features["longitude"] = linear_scale(examples_dataframe["longitude"])
    processed_features["housing_median_age"] = linear_scale(examples_dataframe["housing_median_age"])
    processed_features["total_rooms"] = linear_scale(examples_dataframe["total_rooms"])
    processed_features["total_bedrooms"] = linear_scale(examples_dataframe["total_bedrooms"])
    processed_features["population"] = linear_scale(examples_dataframe["population"])
    processed_features["households"] = linear_scale(examples_dataframe["households"])
    processed_features["median_income"] = linear_scale(examples_dataframe["median_income"])
    processed_features["rooms_per_person"] = linear_scale(examples_dataframe["rooms_per_person"])
    return processed_features

  It gave an RMSE of 71.17 on the training data.

Task 2:

With AdaGrad, the RMSE on the training data was 192.07.

With Adams, the RMSE on the training data was 66.77.

In their solution, they increased the learning rate significantly, so it makes sense that their results were quite different from mine. The RMSE for their AdaGrad was 66.94 and their Adam was 69.96. The got to similar RMSEs, but Ada got there much more quickly.

Task 3:

I tried with z_score_normalize on all of the features. This resulted in a RMSE of 74.59.

Their normalization, choosing fitting options for each of the different features resulted in an RMSE of 70.23, which was better than my versions.

  def normalize(examples_dataframe):
    """Returns a version of the input `DataFrame` that has all its features normalized."""
    processed_features = pd.DataFrame()

    processed_features["households"] = log_normalize(examples_dataframe["households"])
    processed_features["median_income"] = log_normalize(examples_dataframe["median_income"])
    processed_features["total_bedrooms"] = log_normalize(examples_dataframe["total_bedrooms"])

    processed_features["latitude"] = linear_scale(examples_dataframe["latitude"])
    processed_features["longitude"] = linear_scale(examples_dataframe["longitude"])
    processed_features["housing_median_age"] = linear_scale(examples_dataframe["housing_median_age"])

    processed_features["population"] = linear_scale(clip(examples_dataframe["population"], 0, 5000))
    processed_features["rooms_per_person"] = linear_scale(clip(examples_dataframe["rooms_per_person"], 0, 5))
    processed_features["total_rooms"] = linear_scale(clip(examples_dataframe["total_rooms"], 0, 10000))

    return processed_features

  normalized_dataframe = normalize(preprocess_features(california_housing_dataframe))
  normalized_training_examples = normalized_dataframe.head(12000)
  normalized_validation_examples = normalized_dataframe.tail(5000)

  _ = train_nn_regression_model(
      my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.15),
      steps=1000,
      batch_size=50,
      hidden_units=[10, 10],
      training_examples=normalized_training_examples,
      training_targets=training_targets,
      validation_examples=normalized_validation_examples,
      validation_targets=validation_targets)

Task 4: did not exist in this exercise.

c. Optional Challenge: You can skip this exercise.

Successfully skipped.
