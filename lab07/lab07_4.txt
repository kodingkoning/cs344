CS 344, Lab 7, Exercise 3
Elizabeth Koning

a.
Task 1: Examine the Data
Looking at the targets for both the training and the validation, I'm noticing that there are some differences. The mean and standard deviation are both greater for the validation.
I also notice that the units on the income is unclear. The minimum and maximums on the house values are the same for both. Are these artificially there?
There are also some surprising maximums, like 55.2 rooms per person and surprising minimums, like 0 rooms per person. These seem like situations that aren't necessarily comprable to the other blocks.

Task 2: Plot Lat/Long vs. Median House Value
There are significant differences between the two datasets, in both the lat and long, as well as the house values and population.

Task 3: Return and Debug
We are not actually randomizing the permutation of the data. The data is being split in the order that it is in the file, which seems to be sorted by location.

Task 4: Train and Evaluate
Modified portion of model:
  training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)
  training_predictions = np.array([item['predictions'][0] for item in training_predictions])

  validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)
  validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])

parameters for training:
linear_regressor = train_model(
    learning_rate=0.00003,
    steps=500,
    batch_size=5,
    training_examples=training_examples,
    training_targets=training_targets,
    validation_examples=validation_examples,
    validation_targets=validation_targets)

Here, we found the predictions for both datasets from the model based on the inputs. Then, in order to train and use the model, we increased the learning rate, number of steps, and batch size. Overall, each one of these increases help the model predict more accurately.
RMSE for last period: 162.30

Task 5: Evaluate on Test Data


  california_housing_test_data = pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv", sep=",")

  test_examples = preprocess_features(california_housing_test_data)
  test_targets = preprocess_targets(california_housing_test_data)

  predict_test_input_fn = lambda: my_input_fn(
        test_examples,
        test_targets["median_house_value"],
        num_epochs=1,
        shuffle=False)

  test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)
  test_predictions = np.array([item['predictions'][0] for item in test_predictions])

  root_mean_squared_error = math.sqrt(
      metrics.mean_squared_error(test_predictions, test_targets))

  print("Final RMSE (on test data): %0.2f" % root_mean_squared_error)


Final RMSE (on test data): 160.73

Here, we ran the regression model on the test data, and found the mean squared error for each of the predictions.

Our RMSE on our test data was even better than for our validation data. This means that we didn't overfit to the validation dataset and our model generalizes to our entire dataset, and ideally, to the population, if our dataset is a good reflector of the population.

b. Give a one-paragraph summary of what you learned about using training, validation and testing datasets.

In this section, I first saw that it is critical to randomize the data so there won't be systematic bias in the division. Within that, I saw how it looks in the summary statistics to have the data systematically divided. There were quite a few measures across the measurements that were different. The lat/long was the most obvious one. With that, it was very helpful to draw them out on a graph. I don't really have a sense of what one degree in lat or long looks like, so it was hard to tell whether the differences in the mean were significant. Once I saw the plot that looked like a map though, it was quite obvious.

I also learned how to use the validation and testing datasets. As in the guide, the validation set is used over and over to strengthen the parameters, but the test dataset is only used once to compare. In this case, we found that it was still a good fit for the test data, so our training model was successful.
